---
title: A practical guide to geospatial interpolation with R
author: ~
date: '2020-05-21'
slug: spatial_interpolation
categories: []
tags: []
subtitle: ''
summary: ''
authors: []
lastmod: '2020-05-21T18:09:20+02:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
draft: true
---


One of the most exciting things you can to with R is geospatial interpolation. This means that you have some kind of information (e.g. measurements of, say, soil temperature) for a limited number of locations and then you apply a mathematical model that will provide you with an educated guess of what your result might look like, if you would have measured at every possible location. The practical advantage is clear. Sampling is expensive and therefore always limited. This is especially true in the geosciences, where sampling might require hundreds of thousands of Euros for drilling deep holes to the formation of your interest.  
  
  Doing your geospatial interpolation in R is a great opportunity. Many software tools often associated with that task are commercial products that can become fairly expensive, so it is pretty common that if one finishes university or changes a job position, all the work spend on learning that tool is rendered void. R on the other hand is always free and offers a wide range of algorithms of all kinds of flavours. An additional bonus is the seamless integration with the powerful data transformation and visualization capabilities of the language.
  
  A problem when using R (especially at the beginning) is, that the user interface for the application of different algorithms on the many available data structures varies wildly. Although improvements clearly have been made in the recent years, there is little standardization and every package author basically comes up with their own philosophy on how to handle input/output.
  
  As a consequence, starting with geospatial interpolation in R can be frustrating. Of course you can (and will have to) dig through the documentation of the various packages, but as interpolation models are a fairly advanced topic of statistics, those tend to be heavy on technical slang. 
  
  There are a couple of good blog posts (see [here](https://rpubs.com/nabilabd/118172), [here](https://rspatial.org/raster/analysis/4-interpolation.html) and [here](https://fishandwhistle.net/post/2019/bathymetry-lake-volume-estimation-using-r/)) on geospatial interpolation, but they either focus on dealing with specific package interfaces or are again written for an audience that is already well versed in the world of spatial data.
  
  When I started to learn about geospatial interpolation with R a couple of years ago, I found it difficult to get up to speed with real world data. Most use cases I found focuses on pre-treated datasets whose conditions I could not sensibly reproduce with my own data or made steps that seemed outright incomprehensible at that time. So I feel like there is a documentation gap for a practical guide of the very basics of geospatial interpolation: how do I get from having a bunch of field observations to a regular raster of interpolated values that I can plot on a nice little map? I am going to present you here with a minimalistic workflow designed to step-by-step explain the principles behind the necessary operations that all approaches for geospatial interpolation with R have in common. That way I hope your entrance into this fascinating world will be more pleasent than mine was.  
  
  There are four steps to Geospatial interpolation 
  0. Have some data
  1. Create a grid template
  2. Fit a model 
  3. Interpolate!
  
  So let's start!
  
  # Step 0: Have some data
  
```{r show_setup, message=FALSE}

# We will need some packages for (spatial) data processing
library(tidyverse) # wrangling tabular data and plotting
library(sf) # processing spatial vector data
library(sp) # another vector data package necessary for continuity
library(raster) # processing spatial raster data. !!!overwrites dplyr::select!!!

# And a lot of different packages to test their interpolation functions
library(gstat)  # inverse distance weighted, Kriging
library(fields) # Thin Plate Spline
library(interp) # Triangulation
library(mgcv)   # Spatial GAM

# Download the data for this tutorial from Github
# Data is provided by the senate of the city of Berlin under:
# dl-de/by-2-0 Umweltatlas Berlin / Grundwasserg√ºte Ammonium (Umweltatlas)
# for more details see: http://www.stadtentwicklung.berlin.de/umwelt/umweltatlas/ia204.htm

pts_NH4 <- readr::read_csv(
  "https://raw.githubusercontent.com/Ignimbrit/exchange/master/data/2020/Ammonium_Hoppegarten.csv",
  col_types = cols(NH4 = col_double(), 
                   X = col_double(), Y = col_double(), 
                   fid = col_character(), licence = col_character())
  ) %>% 
  dplyr::select(X, Y, NH4)

print(pts_NH4)

```
  
  In the chunk above we already solved Step 0: Have some data! For this tutorial I compiled a little demonstration dataset (see comments for the source) that you can download from github to follow my steps. The data comes from a csv that contains 64 measurements of [Ammonium](https://en.wikipedia.org/wiki/Ammonium) (chemical formula NH4+) in groundwater from a former septic drain field ("Rieselfeld") at a place called Hoppegarten at the outer rim of Berlin. Ammonium in groundwater can be a problem, so it would be good to know exactly how big our problem is, which can hardly be assessed by a couple of randomly scattered points per se.

```{r pts_plot}
point_plot <- ggplot(
  data = pts_NH4,
  mapping = aes(x = X, y = Y, color = NH4)) +
  geom_point(size = 3) +
  scale_color_gradientn(colors = c("blue", "yellow", "red"))

point_plot
```
  
  We can see from plotting the points alone that there seems to be som spatial dependency on the ammonium concentration. Samples with high concentration seem to be sitting mostly at the center of the plot. This is all we need: a coordinate pair (in this context sometimes referred to as the independent variables) and an associated continuous variable (the corresponding dependent variable). If there is a spatial pattern behind the values of the continuous variable (so that it really is depending on the coordinates specifying its position), geospatial interpolation can work.
  
  # Step 1: Create a grid template
  
  This is one of the less intuitive steps but a very important one nonetheless. You need to specify were you would like to interpolate (aka generate new information), before you actually have that new information. When learning about geospatial interpolation with R in the first place, this was the step used to bother me the most. After all: how am I supposed to know where I would like my information, if I do not actually have any for now?
  
  I will not dive any further into the why and hows here. For now just accept that we need a template to later fill it with interpolated values, and that creating this template is super easy, so that you should not worry about it. There are a couple of catches, but we will discuss those as we encounter then. Let's start with a very basic approach to grid template making.
  
  ## Option a) The simple approach
  
```{r simple_gridmaking}
# First let's define a bounding Box, a rectangle that contains all our data
# points. There are many ways to do this but I am keeping it as simple as
# possible on purpose here
bbox <- c(
  "xmin" = min(pts_NH4$X),
  "ymin" = min(pts_NH4$Y),
  "xmax" = max(pts_NH4$X),
  "ymax" = max(pts_NH4$Y)
)

grd_template <- expand.grid(
  X = seq(from = bbox["xmin"], to = bbox["xmax"], by = 20),
  Y = seq(from = bbox["ymin"], to = bbox["ymax"], by = 20) # 20 m resolution
)

grid_plot <- ggplot() +
  geom_point(data = grd_template, aes(x = X, y = Y), size = 0.01) +
  geom_point(data = pts_NH4,
  mapping = aes(x = X, y = Y, color = NH4), size = 3) +
  scale_color_gradientn(colors = c("blue", "yellow", "red")) +
  coord_cartesian( #zooming in so we can actually see something
    xlim = c(408000, 409000), ylim = c(5815000, 5816000))

grid_plot

```
  
  So this is the most simple approach on making a grid_template I could think of. And when I simple I mean that in an absolute positive way. No dependencies, no domain specific knowledge. It has the advantage of needing no other libraries and producing a `data.frame` which is easy to handle. 
  
  ## Option b) the classical approach
  
  Using `base::expand.grid` is not necessarily the best way to build your grid template. Another popular way is to use `sf::st_make_grid`
  
```{r demonstrate_makegrid}
sf_NH4 <- st_as_sf(pts_NH4, coords = c("X", "Y"), crs = 25833)
alt_grd_template_sf <- st_make_grid(
  sf_NH4, cellsize = c(20, 20), what = "centers") %>%
  cbind(., st_coordinates(.))

```

  Those commands will create a fairly similar grid to the one we made above, but there are some differences.
  
  * The `alt_grd_template_sf` does actually contain all sample points. If you've been following my steps closely, you might have noticed, that I made no effort ensuring that when creating `grd_template` in my call to `expand.grid`
  * `alt_grd_template_sf` was created using functions from the `sf`package and the object itself is not a simple `data.frame` but comes in a data structure profided by that package (a `sfc`, or simple feature coloumn, to be specific).
  
  Working with the data structures from the `sf` package is great. They are specifically build to handle spatial vector data and many interpolation algorithms that are out their are explicitly tailored towards accepting them or the closely related structures from the "older-sister-package" `sp`. 
  
  
  # Step 1b: Rasterizing your grid template
  
  I warned you that this tutorial would have to be a bit convoluted. We have our grid template now and it contains all the information we need. Regardless whether you prefer `base::expand.grid` or `sf::st_make_grid`, what you get in return is essentially a table of point coordinates.
  
  For some interpolation algorithms this is just fine as they work well with data.frames. Others don't. Usually there are ways to make it work but often enough things will get messy. As we want to look at a couple of different interpolation methods from different packages with vastly different interfaces and expectations, we will create a copy of our grid template in a different data structure to make our life a little easier.
  
```{r rasterize_template}
grd_template_raster <- grd_template %>% 
  dplyr::mutate(Z = 0) %>% 
  raster::rasterFromXYZ( # {raster} expects a PROJ.4 string, see https://epsg.io/25833
    crs = "+proj=utm +zone=33 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs")


```
  
  # Step 2: Fit a model
  
  Now that we have surpassed the first line of confusion we can progress to the next one. Because even now that We have data to interpolate from and a grid to interpolate to, the process in between does (in most cases) not require one but two steps. The algorithm of our choice must be fitted to our data before we can use that fit to make interpolations. 
  In R those two processes are (usually) cleanly separated. This apparently makes perfect sense to statisticians for which, after all, the whole language was build. For the average geoscience rookie/end user this might be a novel concept and appear unnecessarily cumbersome. Why would we separate fitting of data with interpolating from it? I mean, it's not like we are going to use the fitted model on any other data anyway. But bear with me, it can be done.
  
```{r fitting_models}
# Nearest Neighbor
fit_NN <- gstat::gstat( # using package {gstat} 
  formula = NH4 ~ 1,    # The column `NH4` is what we are interested in
  data = as(sf_NH4, "Spatial"), # using {sf} and converting to {sp}, which is expected
  nmax = 10, nmin = 3 # Number of neighboring observations used for the fit
)

# Inverse Distance Weighting
fit_IDW <- gstat::gstat( # The setup here is quite similar to NN
  formula = NH4 ~ 1,
  data = as(sf_NH4, "Spatial"),
  nmax = 10, nmin = 3,
  set = list(idp = 0.5) # inverse distance power
)

# Triangular Irregular Surface
fit_TIN <- interp::interp( # using {interp}
  x = pts_NH4$X,           # the function actually accepts coordinate vectors
  y = pts_NH4$Y,
  z = pts_NH4$NH4,
  xo = grd_template$X,     # here we already define the target grid
  yo = grd_template$Y,
  output = "points"
)

# Thin Plate Spline Regression
fit_TPS <- fields::Tps( # using {fields}
  x = as.matrix(pts_NH4[, c("X", "Y")]), # accepts points but expects them as matix
  Y = pts_NH4$NH4,  # the dependent variable
  miles = FALSE     # EPSG 25833 is based in meters
)

# Generalized Additive Model
fit_GAM <- mgcv::gam( # using {mgcv}
  NH4 ~ s(X, Y),      # here come our X/Y/Z data - straightforward enough
  data = pts_NH4      # specifiy in which object the data is stored
)

```  
  
```{r sammeln, eval=FALSE}

library(tidyverse)
library(mapview)
library(sf)
library(raster)

pts_NH4 <- readr::read_csv(
  "https://raw.githubusercontent.com/Ignimbrit/exchange/master/data/2020/Ammonium_Hoppegarten.csv",
  col_types = cols(NH4 = col_double(), 
                   X = col_double(), Y = col_double(), 
                   fid = col_character(), licence = col_character())
  ) %>% 
  select(X, Y, NH4)

sf_NH4 <- st_as_sf(pts_NH4, coords = c("X", "Y"), crs = 25833)
mapview(sf_NH4, map.types = "OpenTopoMap")

bbox <- c(
  "xmin" = min(pts_NH4$X),
  "ymin" = min(pts_NH4$Y),
  "xmax" = max(pts_NH4$X),
  "ymax" = max(pts_NH4$Y)
)

grd_template <- expand.grid(
  X = seq(from = bbox["xmin"], to = bbox["xmax"], by = 20),
  Y = seq(from = bbox["ymin"], to = bbox["ymax"], by = 20) # 20 m resolution
)

raster_template <- grd_template %>% 
  mutate(Z = 0) %>% 
  rasterFromXYZ( # {raster} expects a PROJ.4 string, see https://epsg.io/25833
    crs = "+proj=utm +zone=33 +ellps=GRS80 +towgs84=0,0,0,0,0,0,0 +units=m +no_defs")

#########
library(fields)
m <- fields::Tps(x = as.matrix(select(pts_NH4, X, Y)), Y = pts_NH4$NH4, miles = FALSE)
r <- interpolate(raster_template, m)

test <- st_as_sf(pts_NH4, coords = c("X", "Y"), crs = 25833)
test2 <- st_as_sf(mutate(grd_template, id = seq(1, nrow(grd_template))), coords = c("X", "Y"), crs = 25833)
```

